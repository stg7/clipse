\documentclass{article}

\usepackage{arxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{color}
\usepackage{hyperref}
\hypersetup{colorlinks,citecolor=red,linkcolor=blue,urlcolor=cyan}
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{cleveref}       % smart cross-referencing
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{doi}
\usepackage{listings}
\graphicspath{{./imgs/}}

\title{CLIPSE -- a minimalistic CLIP-based image search engine for research}

% Here you can change the date presented in the paper title
%\date{September 9, 1985}
% Or remove it
%\date{}

\newif\ifuniqueAffiliation
% Comment to use multiple affiliations variant of author block
\uniqueAffiliationtrue


\author{ Steve Göring\hspace{1mm}\href{https://orcid.org/0000-0001-6810-6969}{\includegraphics[scale=0.06]{orcid.pdf}}\\
    Audiovisual Technology Group\\
    Technische Universität Ilmenau\\
    Germany \\
    \texttt{steve.goering@tu-ilmenau.de} \\
}


% Uncomment to override  the `A preprint' in the header
%\renewcommand{\headeright}{Technical Report}
%\renewcommand{\undertitle}{Technical Report}
\renewcommand{\shorttitle}{\textit{arXiv} Template}


\hypersetup{
pdftitle={CLIPSE -- a minimalistic CLIP-based image search engine for research},
pdfsubject={image search engine},
pdfauthor={Steve Göring},
pdfkeywords={image search},
}

\begin{document}
\maketitle

\begin{abstract}

\end{abstract}


% keywords can be removed
\keywords{image search \and CLIP}


\section{Introduction}
Considering the increase of uploaded images per year, e.g., to photo sharing platforms (Flickr, Instagram, \ldots), or using AI text-to-image generators (DALL-E, Midjourney, \ldots), finding images to match a given text description is still an ongoing challenge.
Various image search engines, e.g., Google search, Bing search, or open-source engines (WISE~\cite{wise}, Image Search Engine~\cite{ise}), are available.
Recent advancements in machine learning move search engines from traditional text-based approaches using provided meta-data for images to use deep learning models.
For example WISE~\cite{wise} and Image Search Engine (ISE)~\cite{ise} both use vision-language models such as OpenCLIP~\cite{ilharco_gabriel_2021_5143773,cherti2023reproducible,Radford2021LearningTV,schuhmann2022laionb}.
The images for the search engine are transformed to one reduced embedding space, based on pre-training~\cite{Radford2021LearningTV}.
The pre-trained model, was trained with (image, text) pairs, so that even an arbitrary text can be transformed to this embedded space.
However, especially for research, small and easy to setup search engines considering own datasets are hard to find.
\textcolor{red}{Usually image search engines rely on estimated meta-data or manually annotations (tags) cite Information retrieval paper.}
For this reason, a minimalistic search engine, called CLIPSE (CLIP-based image Search Engine), was developed.
CLIPSE is published as open-source software\footnote{\url{clipse}} and builds on Python~3, and HTML5 with CSS and Javascript.
The design follows a simplification approach, therefor everything is kept as simple as possible.
This enables to perform a simple text-based image search for a given data-set, e.g., using command line or with a web-interface.



\section{Overview}
Similar to WISE and ISE, CLIPSE extracts for all images of a given data-set embeddings from OpenCLIP.
CLIPSE is a Python~3 application, and the setup is kept simple, UV~\cite{uv}(a fast and simple Python package manager) will automatically configure the dependencies (e.g. \texttt{pandas}, \texttt{flask}, \texttt{rich}, \texttt{tqdm}, and \texttt{open\_clip}).
CLIPSE further can process images using only CPU, therefore does not require a GPU.

As first step, the index must be created, here \lstinline[language={bash}]{build_index.py} can be called, with the folder of the images, it is recommended to resize them to e.g. $480\times480$ (or similar according to the aspect ratio) for faster processing.
The image is stored as \texttt{json} (for exploration) and compressed \lstinline[language={python}]{numpy.npz} (for faster loading).
After the index is created and stored, it can be accessed with two possible ways.
The first one is using the command-line \lstinline[language={bash}]{query.py} and produces colored output using the \texttt{rich} package.
A possible output is shown in Figure~\ref{fig:cli}.
\lstinline[language={bash}]{query.py} has several modes, e.g., for batch processing (no colors) or interactive sessions.

\begin{figure}
\centering
\includegraphics[width=0.5\textwidth]{CLI.jpg}
\caption{CLIPSE command line interface, example query and result.}
\label{fig:cli}
\end{figure}

The second possible interfacing way is based on a web technology \lstinline[language={bash}]{server.py}.
An example for the web interface is shown in Figure~\ref{fig:web}.

\begin{figure}
\centering
\includegraphics[width=0.85\textwidth]{WEB.jpg}
\caption{CLIPSE web interface, example query and result.}
\label{fig:web}
\end{figure}

The web-interface is based on \texttt{flask} and one simple template which uses a minimal CSS framework (\texttt{MVP}) and two JavaScript functions (one to perform the server request to gather the search results, and another one for the pagination functionality).

In both interfacing cases, the text query is transformed using OpenCLIP to the same embedding space.
Afterwards, the similarity of the text considering all images is calculated, based on the dot product.
The used dot product can be seen as an unnormalized cosine similarity measure.
Based on the similarity scores, the results are sorted and delivered as output to the corresponding interface.
Thus the performance of CLIPSE depends on the used embedding model, which can be changed in the python code.
Overall, to extend CLIPSE for very large datasets several instances could run for subsets and then a final aggregation can be performed of the results.

\section{Benchmark}
In the following section, small proof-of-concept benchmarks are performed, to evaluate the time to index a dataset and the time to perform queries.
All benchmarks are performed on a Intel NUC with an Intel Core i7-1185G7 CPU, 64 GB main memory, a fast SSD, and no GPU acceleration.
PyTorch does parallel processing, however the system performs indexing and querying in a sequential manner.

Three test datasets are used:
\begin{itemize}
    \item \texttt{avt\_ai\_images}: 146 images based on text-to-image generation~\cite{goering2023ai,goering2023aiquality}
    \item \texttt{sample200}: 200 randomly selected images from the Unsplash-lite dataset~\cite{unsplash}
    \item \texttt{div2k}: all 900 images from the DIV2K dataset~\cite{agustsson2017ntire} (train and validation)
    \item \texttt{sophoappeal}: 1061 images from the SoPhoAppeal dataset~\cite{goering2023imageappeal}
    \item \texttt{avt\_image\_db}: 1133 images from~\cite{goering2019Intra}
    \item \texttt{unsplash-lite}: the full Unsplash-lite dataset~\cite{unsplash} consisting of $\approx 25k$ images
\end{itemize}

The images have been resized before, to a resolution of $480\times W$ with $W$ adjusted to the aspect ratio of the image.
For all time measurements 32 repetitions have been performed.

\subsection{Time to index}

\begin{table}[htb!]
\caption{Measured time to index the datasets.}
\label{tbl:measured_time_to_index_the_datasets_}
\begin{tabular}{llllr}
\toprule
dataset                  & \# images & average time [s]   & std time [s]       & avg time per image [s] \\
\midrule
\texttt{avt\_ai\_images} & 146       &                    &                    & \\
\texttt{sample200}       & 200       &                    &                    & \\
\texttt{div2k}           & 900       &                    &                    & \\
\texttt{sophoappeal}     & 1061      &                    &                    & \\
\texttt{avt\_image\_db}  & 1133      &                    &                    & \\
\texttt{unsplash-lite}   & 24976     &                    &                    & \\
\bottomrule
\end{tabular}
\end{table}

The build index step consists of creating the embeddings, and then reading the index as a test and also to further convert the json file to a numpy file (for faster storing), the last step was deactivated for the measurements.
In Table~\ref{tbl:measured_time_to_index_the_datasets_} ....

\subsection{Time to process queries}

query = "a cat exploring the dark night"

\section{Conclusion}


\bibliographystyle{alpha}
\bibliography{refs}



\end{document}
